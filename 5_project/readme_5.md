Task 1
Назначь колонку ID индексом таблицы. После выведите 5 первых строк таблицы.

Task 2
Определимся что является нашим X, а что является нашим Y.
Для этого изучи документ, описывающий наш датасет.
Признаки сохрани в переменную X. Целевую переменную, которую мы будем предсказывать, в переменную Y.
Выведи размерность этих таблиц.

Task 3
Наши признаки содержат в себе как количественные, так и категориальные переменные. Разделим признаки.
В этом поможет тип данных колонки. Категориальные переменные сохрани в переменную X_cat, а количественные в X_num.
Выведи число категориальных и количественные переменных.

Task 4
Предобработаем количественные переменные. Проведите стандартизацию для признаков X_num.
Сохрани результат стандартизации в переменную X_num_scal.
Выведи среднее и стандартное отклонение по датасету для каждого признака после стандартизации.
Значения округли для сотых.

Task 5
Алгоритмы машинного обучения плохо работают с категориальными признаками, поэтому их надо трансформировать.
Прочитай про One-Hot Encoding подход к кодированию и примени его с нашим категориальным признакам, сохрани результат в переменную X_cat_ohe. Перед этим удали признак CLNT_JOB_POSITION (у него слишком много значений).
Выведи количество колонок таблицы X_cat_ohe

Task 6
Объедини 2 предобработанных датасета. Результат запиши в переменную X_ready. Сохрани датасет в папку datasets/data_prepared.csv.
Подели получившийся датасет на train и test. Процент тестовой выборки: 20%. Используй параметр random_state=21.
Выведи размерность обучающей выборки X_train.

Task 7
Обучи логистическую регрессию с параметрами: solver='liblinear', fit_intercept=False, penalty='l1', random_state=21.
Посчитай accuracy модели на тестовой выборке, сравнив предсказанные значения с реальными.
Выведи значение accuracy модели на тестовой выборке.

Task 8
Посчитай процент клиентов в тестовой выборке, у которых индикатор оттока равен 0, и сравнить с accuracy.
Тебе что-то должно показаться подозрительным)

Task 9
Построй график, на котором видны топ-10 самых важных факторов по мнению модели в абсолютном значении.
В этом тебе поможет функция feature_importanse.
